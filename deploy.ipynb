{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "administrator_access\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"AWS_PROFILE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-tritonserver:22.05-py3\".format(\n",
    "    account_id=os.environ[\"AWS_ACCOUNT_ID\"], region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our LLM, Llama, was downloaded via https://github.com/facebookresearch/llama?fbclid=IwAR3DJH6IFhcRGQJGRjUFFV1aXMjGJxXqMy5O_A9Hs1aNrl3Tdm5bUIHfSrA\n",
    "# since it was easier to run Meta's provided shell script than download from Huggingface. The 7billion parameter model was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 506/506 [00:00<00:00, 389kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 4.95MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 20.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 463kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipConfig, BlipForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "file_path = \"models/blip/1/model.pt\"\n",
    "\n",
    "torch.save(model, file_path)\n",
    "# model = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive model and upload to S3\n",
    "! tar -C models -czf blip.tar.gz blip\n",
    "sagemaker_session.upload_data(path=\"blip.tar.gz\", key_prefix=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = \"s3://sagemaker-us-east-1-295763662460/models/blip.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unknown options: login,--username,AWS,--password-stdin,295763662460.dkr.ecr.us-east-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region us-east-1\n",
    "! docker login --username AWS --password-stdin 295763662460.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -t inference . \n",
    "! docker push 295763662460.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver_22.05-py3:0.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: The execution role ARN \"arn:aws:iam::295763662460:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_llm_img_captioning_bdddeec369a566a3\" is invalid. Please ensure that the role exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m sm_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblip-pt-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mgmtime())\n\u001b[1;32m      3\u001b[0m container \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\"\u001b[39m: triton_image_uri,\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mModelDataUrl\u001b[39m\u001b[39m\"\u001b[39m: model_uri,\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mEnvironment\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mSAGEMAKER_TRITON_DEFAULT_MODEL_NAME\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mblip\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m create_model_response \u001b[39m=\u001b[39m sm_client\u001b[39m.\u001b[39;49mcreate_model(\n\u001b[1;32m     10\u001b[0m     ModelName\u001b[39m=\u001b[39;49msm_model_name, ExecutionRoleArn\u001b[39m=\u001b[39;49mrole, PrimaryContainer\u001b[39m=\u001b[39;49mcontainer\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Arn: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m create_model_response[\u001b[39m\"\u001b[39m\u001b[39mModelArn\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: The execution role ARN \"arn:aws:iam::295763662460:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_llm_img_captioning_bdddeec369a566a3\" is invalid. Please ensure that the role exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\"."
     ]
    }
   ],
   "source": [
    "sm_model_name = \"blip-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"blip\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part is commented out since the model is too big for \n",
    "\n",
    "# bs = 1\n",
    "# width = 256\n",
    "# height = 256\n",
    "# channels = 3\n",
    "# dummy_inputs = torch.rand(bs, channels, width, height).to(device)\n",
    "# model = model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "# traced_model = torch.jit.trace(model, dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface integration with Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::295763662460:role/sagemaker_execution_role\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serializers import DataSerializer, IdentitySerializer\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'yesidcanoc/image-captioning-swin-tiny-distilgpt2', # model_id from hf.co/models\n",
    "  'HF_TASK':'image-to-text' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "# TODO: Make this a realtime model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26\", # transformers version used\n",
    "   pytorch_version=\"1.13\", # pytorch version used\n",
    "   py_version=\"py39\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "content_type = \"image/png\"\n",
    "serializer = IdentitySerializer(content_type=content_type)\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(max_concurrency=4, memory_size_in_mb=3072)\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   serverless_inference_config=serverless_config,\n",
    "   serializer=serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027str\\u0027 object has no attribute \\u0027decode\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-09-29-21-02-47-788 in account 295763662460 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata/img2.png\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m      7\u001b[0m     f \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> 8\u001b[0m     res \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49m\u001b[39mbytearray\u001b[39;49m(f))\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:185\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    178\u001b[0m     data,\n\u001b[1;32m    179\u001b[0m     initial_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     custom_attributes,\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"\\u0027str\\u0027 object has no attribute \\u0027decode\\u0027\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-09-29-21-02-47-788 in account 295763662460 for more information."
     ]
    }
   ],
   "source": [
    "# Trying with byte representation of images\n",
    "# with open(\"data/img1_medium.png\", \"rb\") as data_file:\n",
    "#   image_data = data_file.read()\n",
    "img_url = \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n",
    "img = Image.open(\"data/img1_medium.png\")\n",
    "with open(\"data/img2.png\", \"rb\") as image:\n",
    "    f = image.read()\n",
    "    res = predictor.predict(data=bytearray(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 78.4k/78.4k [00:00<00:00, 11.4MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 507M/507M [00:10<00:00, 47.3MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 241/241 [00:00<00:00, 205kB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"yesidcanoc/image-captioning-swin-tiny-distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A mountain view with trees and mountains in the background.        '}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "url_image = Image.open(requests.get(url, stream=True).raw)\n",
    "local_image = Image.open(\"data/img2.png\")\n",
    "text = \"A picture of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"inputs\": {\n",
    "    \"text\": text,\n",
    "    \"images\": url_image\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object of type <class 'PIL.PngImagePlugin.PngImageFile'> is not Data serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mlocal_image)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:177\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     custom_attributes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m            as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_request_args(\n\u001b[1;32m    178\u001b[0m         data,\n\u001b[1;32m    179\u001b[0m         initial_args,\n\u001b[1;32m    180\u001b[0m         target_model,\n\u001b[1;32m    181\u001b[0m         target_variant,\n\u001b[1;32m    182\u001b[0m         inference_id,\n\u001b[1;32m    183\u001b[0m         custom_attributes,\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39msagemaker_runtime_client\u001b[39m.\u001b[39minvoke_endpoint(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_args)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:231\u001b[0m, in \u001b[0;36mPredictor._create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m custom_attributes:\n\u001b[1;32m    229\u001b[0m     args[\u001b[39m\"\u001b[39m\u001b[39mCustomAttributes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m custom_attributes\n\u001b[0;32m--> 231\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserializer\u001b[39m.\u001b[39;49mserialize(data)\n\u001b[1;32m    233\u001b[0m args[\u001b[39m\"\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_serializers.py:396\u001b[0m, in \u001b[0;36mDataSerializer.serialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    394\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[0;32m--> 396\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m is not Data serializable.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Object of type <class 'PIL.PngImagePlugin.PngImageFile'> is not Data serializable."
     ]
    }
   ],
   "source": [
    "res = predictor.predict(data=local_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
