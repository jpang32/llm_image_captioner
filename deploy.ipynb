{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface integration with Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::295763662460:role/sagemaker_execution_role\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEGACY CODE: Deploying with HuggingFace container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-295763662460/custom_inference/image-captioning-swin-tiny-distilgpt2/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_id = 'yesidcanoc/image-captioning-swin-tiny-distilgpt2'\n",
    "model_name=model_id.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/custom_inference/{model_name}/model.tar.gz\"\n",
    "print(s3_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a src\n",
      "a src/.DS_Store\n",
      "a src/code\n",
      "a src/code/__init__.py\n",
      "a src/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar zcvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-295763662460/custom_inference/image-captioning-swin-tiny-distilgpt2/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "\n",
    "# The \"HF_MODEL_ID\" env var should cause the huggingface container to look\n",
    "# for the model within HF's repository, rather than within our custom\n",
    "# inference artifact\n",
    "\n",
    "# This will allow us to override inference.py without needing to download\n",
    "# the model locally and then re-upload it\n",
    "env_vars = {\n",
    "  'HF_MODEL_ID':model_id, # model_id from hf.co/models\n",
    "  'HF_TASK':'image-to-text', # NLP task you want to use for predictions\n",
    "  'DEVICE': 'cpu',\n",
    "  'S3_MODEL_LOCATION': s3_location\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "  #  env=env_vars,\n",
    "   model_data=s3_location,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26\", # transformers version used\n",
    "   pytorch_version=\"1.13\", # pytorch version used\n",
    "   py_version=\"py39\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "content_type = \"image/png\"\n",
    "serializer = IdentitySerializer(content_type=content_type)\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(max_concurrency=4, memory_size_in_mb=3072)\n",
    "\n",
    "# deploy model to SageMaker Serverless Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   serverless_inference_config=serverless_config,\n",
    "   serializer=serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"(\\\"You need to define one of the following [\\u0027audio-classification\\u0027, \\u0027automatic-speech-recognition\\u0027, \\u0027feature-extraction\\u0027, \\u0027text-classification\\u0027, \\u0027token-classification\\u0027, \\u0027question-answering\\u0027, \\u0027table-question-answering\\u0027, \\u0027visual-question-answering\\u0027, \\u0027document-question-answering\\u0027, \\u0027fill-mask\\u0027, \\u0027summarization\\u0027, \\u0027translation\\u0027, \\u0027text2text-generation\\u0027, \\u0027text-generation\\u0027, \\u0027zero-shot-classification\\u0027, \\u0027zero-shot-image-classification\\u0027, \\u0027conversational\\u0027, \\u0027image-classification\\u0027, \\u0027image-segmentation\\u0027, \\u0027image-to-text\\u0027, \\u0027object-detection\\u0027, \\u0027zero-shot-object-detection\\u0027, \\u0027depth-estimation\\u0027, \\u0027video-classification\\u0027] as env \\u0027HF_TASK\\u0027.\\\", 403)\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-10-09-21-01-35-947 in account 295763662460 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/deploy.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/deploy.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata/img2.png\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/deploy.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     f \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/deploy.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     res \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49m\u001b[39mbytearray\u001b[39;49m(f))\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:185\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    178\u001b[0m     data,\n\u001b[1;32m    179\u001b[0m     initial_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     custom_attributes,\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"(\\\"You need to define one of the following [\\u0027audio-classification\\u0027, \\u0027automatic-speech-recognition\\u0027, \\u0027feature-extraction\\u0027, \\u0027text-classification\\u0027, \\u0027token-classification\\u0027, \\u0027question-answering\\u0027, \\u0027table-question-answering\\u0027, \\u0027visual-question-answering\\u0027, \\u0027document-question-answering\\u0027, \\u0027fill-mask\\u0027, \\u0027summarization\\u0027, \\u0027translation\\u0027, \\u0027text2text-generation\\u0027, \\u0027text-generation\\u0027, \\u0027zero-shot-classification\\u0027, \\u0027zero-shot-image-classification\\u0027, \\u0027conversational\\u0027, \\u0027image-classification\\u0027, \\u0027image-segmentation\\u0027, \\u0027image-to-text\\u0027, \\u0027object-detection\\u0027, \\u0027zero-shot-object-detection\\u0027, \\u0027depth-estimation\\u0027, \\u0027video-classification\\u0027] as env \\u0027HF_TASK\\u0027.\\\", 403)\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/huggingface-pytorch-inference-2023-10-09-21-01-35-947 in account 295763662460 for more information."
     ]
    }
   ],
   "source": [
    "# Trying with byte representation of images\n",
    "# with open(\"data/img1_medium.png\", \"rb\") as data_file:\n",
    "#   image_data = data_file.read()\n",
    "img_url = \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n",
    "img = Image.open(\"data/img1_medium.png\")\n",
    "with open(\"data/img2.png\", \"rb\") as image:\n",
    "    f = image.read()\n",
    "    res = predictor.predict(data=bytearray(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 78.4k/78.4k [00:00<00:00, 11.4MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 507M/507M [00:10<00:00, 47.3MB/s] \n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 241/241 [00:00<00:00, 205kB/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"yesidcanoc/image-captioning-swin-tiny-distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaredpangallozzi/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A mountain view with trees and mountains in the background.        '}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "url_image = Image.open(requests.get(url, stream=True).raw)\n",
    "local_image = Image.open(\"data/img2.png\")\n",
    "text = \"A picture of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"inputs\": {\n",
    "    \"text\": text,\n",
    "    \"images\": url_image\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object of type <class 'PIL.PngImagePlugin.PngImageFile'> is not Data serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(data\u001b[39m=\u001b[39;49mlocal_image)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:177\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     custom_attributes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m            as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_request_args(\n\u001b[1;32m    178\u001b[0m         data,\n\u001b[1;32m    179\u001b[0m         initial_args,\n\u001b[1;32m    180\u001b[0m         target_model,\n\u001b[1;32m    181\u001b[0m         target_variant,\n\u001b[1;32m    182\u001b[0m         inference_id,\n\u001b[1;32m    183\u001b[0m         custom_attributes,\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39msagemaker_runtime_client\u001b[39m.\u001b[39minvoke_endpoint(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_args)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_predictor.py:231\u001b[0m, in \u001b[0;36mPredictor._create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m custom_attributes:\n\u001b[1;32m    229\u001b[0m     args[\u001b[39m\"\u001b[39m\u001b[39mCustomAttributes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m custom_attributes\n\u001b[0;32m--> 231\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserializer\u001b[39m.\u001b[39;49mserialize(data)\n\u001b[1;32m    233\u001b[0m args[\u001b[39m\"\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/PycharmProjects/llm_image_captioner/venv/lib/python3.11/site-packages/sagemaker/base_serializers.py:396\u001b[0m, in \u001b[0;36mDataSerializer.serialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    394\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[0;32m--> 396\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m is not Data serializable.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Object of type <class 'PIL.PngImagePlugin.PngImageFile'> is not Data serializable."
     ]
    }
   ],
   "source": [
    "res = predictor.predict(data=local_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
